{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = datasets.load_iris()\n",
    "\n",
    "X = iris_data.data\n",
    "y = iris_data.target.astype(\"float\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, question, true_branch, false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "    \n",
    "class LeafNode:\n",
    "    def __init__(self, X, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        self.predictions = dict(zip(classes, counts))    \n",
    "\n",
    "class SplitQuestion:\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "    \n",
    "    def match(self, X):\n",
    "        # Compare the feature value in an example to the\n",
    "        # feature value in this question.\n",
    "        matches = X[:, self.column]\n",
    "        return matches >= self.value\n",
    "    \n",
    "    def match_row(self, example):\n",
    "        val = example[self.column]\n",
    "        return val >= self.value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.is_built = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def gini(X, y):\n",
    "        n = float(len(X))\n",
    "        classes, counts = np.unique(y_train, return_counts=True)\n",
    "        class_counts = dict(zip(classes, counts))\n",
    "        impurity = 1\n",
    "        for cl, cnt in class_counts.items():\n",
    "            prob_of_lbl = cnt / n\n",
    "            impurity -= prob_of_lbl**2\n",
    "        \n",
    "        return impurity\n",
    "    \n",
    "    def partition(self, X, y, question):\n",
    "        matches = question.match(X)\n",
    "        \n",
    "        true_X, true_y = X[matches], y[matches]\n",
    "        false_X, false_y = X[~matches], y[~matches]\n",
    "\n",
    "        return true_X, true_y, false_X, false_y\n",
    "    \n",
    "    def info_gain(self, true_X, true_y, false_X, false_y, current_uncertainty):\n",
    "        num_true = len(true_X)\n",
    "        num_false = len(false_X)\n",
    "        p = float(num_true) / (num_true + num_false)\n",
    "        return current_uncertainty - p * self.gini(true_X, true_y) - (1 - p) * self.gini(false_X, false_y)\n",
    "        \n",
    "    def find_best_split(self, X, y):\n",
    "        best_gain = 0\n",
    "        best_question = None\n",
    "        current_uncertainty = self.gini(X, y)\n",
    "        n_features = X.shape[1]\n",
    "        ret_true_X, ret_true_y, ret_false_X, ret_false_y = None, None, None, None\n",
    "\n",
    "        for col in range(n_features):\n",
    "            unique_vals = np.unique(X[:, col])\n",
    "            for val in unique_vals:\n",
    "                question = SplitQuestion(col, val)\n",
    "\n",
    "                # try splitting the dataset\n",
    "                true_X, true_y, false_X, false_y = self.partition(X, y, question)\n",
    "\n",
    "                # skip if data are not split\n",
    "                if len(true_X) == 0 or len(false_X) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = self.info_gain(true_X, true_y, false_X, false_y, current_uncertainty)\n",
    "\n",
    "                # Can be either '>' or '>='\n",
    "                if gain >= best_gain:\n",
    "                    best_gain, best_question = gain, question\n",
    "                    ret_true_X, ret_true_y, ret_false_X, ret_false_y = true_X, true_y, false_X, false_y\n",
    "\n",
    "        return best_gain, best_question, ret_true_X, ret_true_y, ret_false_X, ret_false_y\n",
    "    \n",
    "    def build_tree(self, X, y):\n",
    "        gain, question, true_X, true_y, false_X, false_y = self.find_best_split(X, y)\n",
    "        \n",
    "        if gain == 0:\n",
    "            return LeafNode(X, y)\n",
    "        \n",
    "        # Recursively build the true branch.\n",
    "        true_branch = self.build_tree(true_X, true_y)\n",
    "\n",
    "        # Recursively build the false branch.\n",
    "        false_branch = self.build_tree(false_X, false_y)\n",
    "\n",
    "        # Return a DecisionNode node.\n",
    "        # This records the best feature / value to ask at this point,\n",
    "        # as well as the branches to follow\n",
    "        # depending on the answer.\n",
    "        return DecisionNode(question, true_branch, false_branch)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self.build_tree(X, y)\n",
    "        self.is_built = True\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            y_p = self.classify(x, self.model)\n",
    "            y_p = list(y_p.values())\n",
    "            y_pred.append(y_p)\n",
    "            \n",
    "        return np.concatenate(y_pred, axis=0)\n",
    "        \n",
    "    def classify(self, x, node):\n",
    "        if isinstance(node, LeafNode):\n",
    "            return node.predictions\n",
    "\n",
    "        # Decide whether to follow the true-branch or the false-branch.\n",
    "        # Compare the feature / value stored in the node,\n",
    "        # to the example we're considering.\n",
    "        if node.question.match_row(x):\n",
    "            return self.classify(x, node.true_branch)\n",
    "        else:\n",
    "            return self.classify(x, node.false_branch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = tree_clf.predict(X_test)\n",
    "f1_score(y_test, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

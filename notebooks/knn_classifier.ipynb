{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the simple Iris dataset which is perfect for binary classification problems. For the sake of simplicity we will only use the first two feature columns. Scaling is not really required for K-NN since the relative differences will stay the dame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features.\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have multiple ways of implementing searching for the KNNs. Here we go with the simple approach that does not require fitting and computes all distances on the flight and selects the K closest points. Alternatively, we could make use of a K-D tree in order to make the similarity search more efficient but this would require first building the tree before making predictions.\n",
    "\n",
    "In order to select the predicted class we use majority voting over the retrieved KNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def predict(self, X_train, y_train, X_test):\n",
    "        X_dist = distance_matrix(X_train, X_test).T\n",
    "        \n",
    "        idx = np.argpartition(X_dist, self.k, axis=1)    # k first elements will be the smallest\n",
    "        y_nn = y_train[idx[:, :self.k]]    # labels of k nearest training samples\n",
    "        bin_counts = np.apply_along_axis(np.bincount,\n",
    "                                         axis=1,\n",
    "                                         arr=y_nn,\n",
    "                                         minlength=np.max(y_nn) + 1)\n",
    "        y_pred = np.argmax(bin_counts, axis=1)    # majority voting (bincount works for nonnegative int values)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNNClassifier(k=5)\n",
    "y_pred = knn_clf.predict(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to make predictions for the test data and compute the f1 score over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
